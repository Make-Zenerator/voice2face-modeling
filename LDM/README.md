
  

# ğŸ”Š Voice2Face-modeling

  

<img  src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=Python&logoColor=white"> <img  src="https://img.shields.io/badge/opencv-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white"> <img  src="https://img.shields.io/badge/git-F05032?style=for-the-badge&logo=git&logoColor=white"> <img  src="https://img.shields.io/badge/NCP-03C75A?style=for-the-badge&logo=Naver&logoColor=white"> <img  src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=Linux&logoColor=white"> <img  src="https://img.shields.io/badge/Numpy-013243?style=for-the-badge&logo=Numpy&logoColor=white"> <img  src="https://img.shields.io/badge/Pytorch-EE4C2C?style=for-the-badge&logo=Pytorch&logoColor=white"> <img  src="https://img.shields.io/badge/Huggingface-FFD21E?style=for-the-badge&logo=Huggingface&logoColor=white">



## Project Structure
```
code
â”£ config
â”ƒ â”£ predict_template.yaml
â”ƒ â”£ train.yaml
â”ƒ â”— train_template.yaml
â”£ models
â”ƒ â”£ diffusion
â”ƒ â”ƒ â”— diffusion_models.py
â”ƒ â”£ module
â”ƒ â”ƒ â”£ attention.py
â”ƒ â”ƒ â”£ blocks.py
â”ƒ â”ƒ â”£ decoder.py
â”ƒ â”ƒ â”£ ema.py
â”ƒ â”ƒ â”£ encoder.py
â”ƒ â”ƒ â”£ noise_scheduler.py
â”ƒ â”ƒ â”£ voice_encoder.py
â”ƒ â”ƒ â”— vqvae.py
â”ƒ â”— utils.py
â”£ modules
â”ƒ â”£ datasets.py
â”ƒ â”£ earlystoppers.py
â”ƒ â”£ losses.py
â”ƒ â”£ metrics.py
â”ƒ â”£ optimizers.py
â”ƒ â”£ recoders.py
â”ƒ â”£ scalers.py
â”ƒ â”£ schedulers.py
â”ƒ â”£ trainer.py
â”ƒ â”£ utils.py
â”ƒ â”— vad_ex.py
â”£ LDM_LoRA.ipynb
â”£ LDM_models.py
â”£ LoRA_train.py
â”£ custom_pipeline.py
â”£ inference.py
â”£ logger.ipynb
â”£ logger.py
â”£ predict.py
â”£ requirements.txt
â”£ sf2f_encoder.py
â”— train.py
```

## Usage

 - Latent Diffusion: [paper](https://arxiv.org/abs/2112.10752) | [github](https://github.com/CompVis/latent-diffusion?tab=readme-ov-file)
 - Low-Rank Adaptation: [paper](https://arxiv.org/abs/2106.09685) | [github](https://github.com/microsoft/LoRA)
  

#### config
 - ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëª¨ì•„ë‘” í´ë”ì…ë‹ˆë‹¤.
 - `train.yaml`: trainì— ì‚¬ìš©ë˜ëŠ” ë§¤ê°œ ë³€ìˆ˜ë“¤ì„ ëª¨ì•„ë‘ì–´ ìˆ˜ì • ë° í™•ì¸ì´ ìš©ì´í•˜ê²Œ í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

#### models
 - `diffusion/diffusion_models.py`: diffusion modelì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ê³  config fileì„ í†µí•´ ì „ë‹¬ë°›ì€ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ buildí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/attention.py`: diffusion modelì—ì„œ ë™ì‘í•˜ëŠ” attention ë§¤ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/blocks.py`: diffusion modelì„ êµ¬ì„±í•  ë•Œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ blockë“¤ì˜ í˜•íƒœë¥¼ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/decoder.py`: diffusion modelì—ì„œ encoderì—ì„œ ë°›ì•„ì˜¨ ë°ì´í„°ë¥¼ ë³µì›í•˜ê¸° ìœ„í•œ decoder êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/ema.py`: exponential moving averageë¥¼ í†µí•´ noiseë¥¼ ì¤„ì—¬ ì„±ëŠ¥ì„ í–¥ìƒí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/encoder.py`: diffusion modelì˜ encoderë¡œ ë°ì´í„°ì˜ conditionì„ ì…ë ¥ë°›ì„ ìˆ˜ ìˆë„ë¡ ì •ì˜ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/voice_encoder.py`: ê¸°ì¡´ CLIP encoderë¥¼ ëŒ€ì²´í•˜ì—¬ ìŒì„± ë°ì´í„°(.wav) í˜¹ì€ ì´ë¯¸ì§€(mel_spectrogram, mfcc ë“±)ì„ ì…ë ¥ë°›ëŠ” ì¸ì½”ë” êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `module/vqvae.py`: diffusion modelì˜ í•™ìŠµ ì†ë„ í–¥ìƒì„ ìœ„í•´ vactor quantizationì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê³ , ì´ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µêµ¬í•˜ê¸° ìœ„í•œ vae êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - 
#### modules 
 - `dataset.py`: ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” datasetì„ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `earlystoppers.py`: overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” earlystopping ê¸°ë²•ì„ ì ìš©í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `losses.py`: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” loss funtionì„ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `metrics.py`: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” metric(psnr, fid, mssim ë“±)ì„ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `optimizers.py`: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” optimizer(adam, adamw, adamp, RSMproop ë“±)ì„ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `recoders.py`: ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `trainer.py`: trainì—ì„œ ì‚¬ìš©ë  ì •ë³´ë“¤(optimizer, scheduler, scalers ë“±)ì„ ëª¨ë‘ ì •ì˜í•˜ê³  í•™ìŠµ ê³¼ì •ì— ëŒ€í•´ ì„¸ë¶€ ì •ì˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `vad_ex.py`: dataì˜ í˜•íƒœë¥¼ í†µì¼í•˜ê¸° ìœ„í•´ ìŒì„± ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ ë°ì´í„°(mel_spectrogram, mfcc)ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - 
#### total
 - `LDM_LoRA.ipynb`: LDMì˜ í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©ëœ LoRA êµ¬ì¡°ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ ì‘ì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `LDM_models.py`: Pre-trained LDMì„ ì •ì˜í•˜ê³  pipelineì˜ customizingí•˜ê¸° ìœ„í•´ ì‘ì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `LoRA_train.py`: LoRA êµ¬ì¡°ë¥¼ í†µí•´ LDMì„ í•™ìŠµì‹œí‚¤ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `custom_pipeline.py`: Pre-trained ëª¨ë¸ê³¼ voice encoderë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµ pipelineì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ì‘ì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `sf2f_encoder.py`: Pre-trained sf2f voice encoderë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‘ì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
 - `train.py`: LDM ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‘ì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
  

## Getting Started

  
### Setting up Virtual Environment

  
1. Initialize and update the server

```

su -

source .bashrc

```

  

2. Create and Activate a virtual environment in the project directory

  

```

conda create -n env python=3.8

conda activate env

```

  

4. To deactivate and exit the virtual environment, simply run:

  

```

deactivate

```

  

### Install Requirements

  

To Install the necessary packages listed in `requirements.txt`, run the following command while your virtual environment is activated:

```

pip install -r requirements.txt

```

  
  
## Links
- [Naver BoostCamp 6th github](https://github.com/boostcampaitech6/level2-3-cv-finalproject-cv-08)
